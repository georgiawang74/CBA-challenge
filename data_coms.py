# -*- coding: utf-8 -*-
"""Data COMs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wQa_s7ELx7d-IvkQdjv1p84UiQAfbZMB
"""

import yfinance as yf
import pandas as pd

# --- Daily Data ---
tickers = ["GC=F", "HG=F", "CL=F", "NG=F"]  # gold, copper, crude oil, nat gas

data_daily = yf.download(
    tickers,
    start="2020-08-12",
    end="2025-08-12",
    interval="1d",
    auto_adjust=False
)

# Keep only Close and Volume
close_daily = data_daily["Close"].copy()
volume_daily = data_daily["Volume"].copy()

# Rename columns for clarity
close_daily.columns = ["Gold_Close", "Copper_Close", "Oil_Close", "NatGas_Close"]
volume_daily.columns = ["Gold_Volume", "Copper_Volume", "Oil_Volume", "NatGas_Volume"]

# Combine into one dataframe
commodities_daily = pd.concat([close_daily, volume_daily], axis=1)

commodities_daily.to_csv("commodities_daily.csv")
print(commodities_daily.head())

# --- Hourly Data ---
data_hourly = yf.download(
    tickers,
    start="2024-08-12",
    end="2025-08-12",
    interval="1h",
    auto_adjust=False
)

close_hourly = data_hourly["Close"].copy()
volume_hourly = data_hourly["Volume"].copy()

close_hourly.columns = ["Gold_Close", "Copper_Close", "Oil_Close", "NatGas_Close"]
volume_hourly.columns = ["Gold_Volume", "Copper_Volume", "Oil_Volume", "NatGas_Volume"]

commodities_hourly = pd.concat([close_hourly, volume_hourly], axis=1)

commodities_hourly.to_csv("commodities_hourly.csv")
print(commodities_hourly.head())

import pandas as pd

# ===================== DAILY =====================
cdf = pd.read_csv("commodities_daily.csv")

# If MultiIndex from yfinance, keep Close & Volume then flatten
if isinstance(cdf.columns, pd.MultiIndex):
    keep_lv0 = [lvl for lvl in cdf.columns.get_level_values(0).unique() if lvl in ("Close","Volume")]
    cdf = cdf[keep_lv0].copy()
    cdf.columns = [f"{a}_{b}" for a,b in cdf.columns.to_flat_index()]

# Ensure first column is Date
if "Date" not in cdf.columns:
    cdf.rename(columns={cdf.columns[0]: "Date"}, inplace=True)

# Parse dates and sort
cdf["Date"] = pd.to_datetime(cdf["Date"], utc=True, errors="coerce")
cdf = cdf.dropna(subset=["Date"]).set_index("Date").sort_index()

# Map possible column names to explicit *_Close / *_Volume
rename_map = {
    # Close
    "GC=F": "Gold_Close",   "HG=F": "Copper_Close", "CL=F": "Oil_Close",   "NG=F": "NatGas_Close",
    "Gold": "Gold_Close",   "Copper":"Copper_Close","Oil":"Oil_Close",     "NatGas":"NatGas_Close",
    "Close_GC=F":"Gold_Close","Close_HG=F":"Copper_Close","Close_CL=F":"Oil_Close","Close_NG=F":"NatGas_Close",
    # Volume
    "Volume_GC=F":"Gold_Volume","Volume_HG=F":"Copper_Volume","Volume_CL=F":"Oil_Volume","Volume_NG=F":"NatGas_Volume",
    "Gold_Volume":"Gold_Volume","Copper_Volume":"Copper_Volume","Oil_Volume":"Oil_Volume","NatGas_Volume":"NatGas_Volume"
}

cdf = cdf.rename(columns={c: rename_map.get(c, c) for c in cdf.columns})
for c in list(cdf.columns):
    if c in ("GC=F","HG=F","CL=F","NG=F"):
        cdf.rename(columns={c: rename_map.get(c, c)}, inplace=True)

# Collapse duplicate columns
if cdf.columns.duplicated().any():
    cdf = (cdf.T.groupby(level=0).first()).T

# Keep only columns we care about
wanted_daily = ["Gold_Close","Copper_Close","Oil_Close","NatGas_Close",
                "Gold_Volume","Copper_Volume","Oil_Volume","NatGas_Volume"]
present_daily = [c for c in wanted_daily if c in cdf.columns]
cdf = cdf[present_daily].copy()

# Coerce numeric
for col in cdf.columns:
    cdf[col] = pd.to_numeric(cdf[col], errors="coerce")

print(f"[Daily] duplicate timestamps: {int(cdf.index.duplicated().sum())}")
print("[Daily] NaNs per column before fill:\n", cdf.isna().sum().to_dict())

# Drop rows where all are NaN, then fill
cdf = cdf.dropna(how="all")
cdf = cdf.ffill().bfill()

print("[Daily] NaNs per column after fill:\n", cdf.isna().sum().to_dict())

# Save
cdf.to_csv("commodities_daily_clean.csv", index_label="Date")
print(f"[Daily] saved {len(cdf)} rows -> commodities_daily_clean.csv")


# ===================== HOURLY =====================
chf = pd.read_csv("commodities_hourly.csv")

if isinstance(chf.columns, pd.MultiIndex):
    keep_lv0 = [lvl for lvl in chf.columns.get_level_values(0).unique() if lvl in ("Close","Volume")]
    chf = chf[keep_lv0].copy()
    chf.columns = [f"{a}_{b}" for a,b in chf.columns.to_flat_index()]

if "Date" not in chf.columns:
    chf.rename(columns={chf.columns[0]: "Date"}, inplace=True)

chf["Date"] = pd.to_datetime(chf["Date"], utc=True, errors="coerce")
chf = chf.dropna(subset=["Date"]).set_index("Date").sort_index()

chf = chf.rename(columns={c: rename_map.get(c, c) for c in chf.columns})
for c in list(chf.columns):
    if c in ("GC=F","HG=F","CL=F","NG=F"):
        chf.rename(columns={c: rename_map.get(c, c)}, inplace=True)

if chf.columns.duplicated().any():
    chf = (chf.T.groupby(level=0).first()).T

wanted_hourly = ["Gold_Close","Copper_Close","Oil_Close","NatGas_Close",
                 "Gold_Volume","Copper_Volume","Oil_Volume","NatGas_Volume"]
present_hourly = [c for c in wanted_hourly if c in chf.columns]
chf = chf[present_hourly].copy()

for col in chf.columns:
    chf[col] = pd.to_numeric(chf[col], errors="coerce")

print(f"[Hourly] duplicate timestamps: {int(chf.index.duplicated().sum())}")
print("[Hourly] NaNs per column before fill:\n", chf.isna().sum().to_dict())

# Drop all-NaN rows, then fill
chf = chf.dropna(how="all")
chf = chf.ffill().bfill()

print("[Hourly] NaNs per column after fill:\n", chf.isna().sum().to_dict())

chf.to_csv("commodities_hourly_clean.csv", index_label="Date")
print(f"[Hourly] saved {len(chf)} rows -> commodities_hourly_clean.csv")