# -*- coding: utf-8 -*-
"""Data stuff - AUDUSD SPOT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17Jm943xaVUHtbNbYIuOHWIpc1W-RWHXa
"""

import yfinance as yf
import pandas as pd

#Daily data
df = yf.download(
    "AUDUSD=X",
    interval="1d",
    start="2020-08-12",
    end="2025-08-12",
    auto_adjust=False
)

print(df.head(), df.tail(), len(df))
df.to_csv("audusd_daily.csv")

#Hourly Data

df = yf.download("AUDUSD=X", interval="1h", start="2024-08-12", auto_adjust=False)
df.index = pd.to_datetime(df.index, utc=True)
df["Volume"] = df.get("Volume", 0)
df["Source"] = "yfinance"
df = df.rename(columns=str.title)[["Open","High","Low","Close","Volume","Source"]]
df.to_csv("AUDUSD_hourly.csv")

#Data cleaning

# ===== DAILY =====
dfd = pd.read_csv("audusd_daily.csv")

# parse date (assumes first column is the date if not named "Date")
date_col = "Date" if "Date" in dfd.columns else dfd.columns[0]
dfd[date_col] = pd.to_datetime(dfd[date_col], utc=True, errors="coerce")
dfd = dfd.dropna(subset=[date_col]).set_index(date_col)
dfd = dfd.sort_index()

# standardize columns to expected names
rename = {}
for c in dfd.columns:
    lc = c.lower()
    if lc == "open":  rename[c] = "Open"
    if lc == "high":  rename[c] = "High"
    if lc == "low":   rename[c] = "Low"
    if lc == "close": rename[c] = "Close"
    if lc == "adj close": rename[c] = "Adj Close"
    if lc == "volume": rename[c] = "Volume"
    if lc == "source": rename[c] = "Source"
dfd = dfd.rename(columns=rename)

# ensure required columns exist
required = ["Open","High","Low","Close"]
missing = [c for c in required if c not in dfd.columns]
if missing:
    raise ValueError(f"Daily file missing required columns: {missing}")

# add optional columns if missing
if "Volume" not in dfd.columns: dfd["Volume"] = 0
if "Source" not in dfd.columns: dfd["Source"] = "yfinance"

# report duplicates (do NOT drop, just show)
dup_daily = dfd.index.duplicated().sum()
print(f"[Daily] duplicate timestamps: {dup_daily}")

# basic NaN check in OHLC
na_daily = dfd[required].isna().sum()
print("[Daily] NaNs per column:\n", na_daily)

# drop rows with any NaNs in OHLC
dfd = dfd.dropna(subset=required)

# save cleaned daily
dfd.to_csv("audusd_daily_clean.csv", index_label="Date")
print(f"[Daily] saved {len(dfd)} rows -> audusd_daily_clean.csv")



# ===== HOURLY =====
dfh = pd.read_csv("AUDUSD_hourly.csv")

date_col_h = "Date" if "Date" in dfh.columns else dfh.columns[0]
dfh[date_col_h] = pd.to_datetime(dfh[date_col_h], utc=True, errors="coerce")
dfh = dfh.dropna(subset=[date_col_h]).set_index(date_col_h)
dfh = dfh.sort_index()

# standardize columns
rename_h = {}
for c in dfh.columns:
    lc = c.lower()
    if lc == "open":  rename_h[c] = "Open"
    if lc == "high":  rename_h[c] = "High"
    if lc == "low":   rename_h[c] = "Low"
    if lc == "close": rename_h[c] = "Close"
    if lc == "adj close": rename_h[c] = "Adj Close"
    if lc == "volume": rename_h[c] = "Volume"
    if lc == "source": rename_h[c] = "Source"
dfh = dfh.rename(columns=rename_h)

required_h = ["Open","High","Low","Close"]
missing_h = [c for c in required_h if c not in dfh.columns]
if missing_h:
    raise ValueError(f"Hourly file missing required columns: {missing_h}")

if "Volume" not in dfh.columns: dfh["Volume"] = 0
if "Source" not in dfh.columns: dfh["Source"] = "yfinance"

# report duplicates (do NOT drop, just show)
dup_hourly = dfh.index.duplicated().sum()
print(f"[Hourly] duplicate timestamps: {dup_hourly}")

# NaNs in OHLC
na_hourly = dfh[required_h].isna().sum()
print("[Hourly] NaNs per column:\n", na_hourly)

# drop rows with NaNs in OHLC
dfh = dfh.dropna(subset=required_h)


# save cleaned hourly
dfh.to_csv("audusd_hourly_clean.csv", index_label="Date")
print(f"[Hourly] saved {len(dfh)} rows -> audusd_hourly_clean.csv")

#some simple eda

import pandas as pd
import matplotlib.pyplot as plt

daily = pd.read_csv("audusd_daily_clean.csv", parse_dates=["Date"], index_col="Date")
hourly = pd.read_csv("audusd_hourly_clean.csv", parse_dates=["Date"], index_col="Date")


print(daily.info())
print(hourly.info())

print(daily.describe())
print(hourly.describe())


print("Daily range:", daily.index.min(), "to", daily.index.max())
print("Hourly range:", hourly.index.min(), "to", hourly.index.max())


#simple plot

daily["Close"].plot(figsize=(10,4), title="AUDUSD Daily Close (past year)")
plt.show()

hourly["Close"].plot(figsize=(10,4), title="AUDUSD Hourly Close (past year)")
plt.show()

import numpy as np

#Return distribution visual
daily["ret"] = np.log(daily["Close"]).diff()
hourly["ret"] = np.log(hourly["Close"]).diff()

daily["ret"].hist(bins=50)
plt.title("Daily Returns Distribution")
plt.show()

hourly["ret"].hist(bins=50)
plt.title("Hourly Returns Distribution")
plt.show()

#Volatility over time
daily["vol_20d"] = daily["ret"].rolling(20).std()
hourly["vol_24h"] = hourly["ret"].rolling(24).std()

daily["vol_20d"].plot(title="20-Day Rolling Volatility")
plt.show()

hourly["vol_24h"].plot(title="24-Hour Rolling Volatility")
plt.show()

import seaborn as sns

#daily correlation heat map - OHLC (open, high, low, close)
sns.heatmap(daily[["Open","High","Low","Close"]].corr(), annot=True, cmap="coolwarm")
plt.title("Daily OHLC Correlation")
plt.show()

#Hourly Data
import yfinance as yf
import pandas as pd


df = yf.download("AUDUSD=X", interval="1h", start="2025-08-13", auto_adjust=False)
df.index = pd.to_datetime(df.index, utc=True)
df["Volume"] = df.get("Volume", 0)
df["Source"] = "yfinance"
df = df.rename(columns=str.title)[["Open","High","Low","Close","Volume","Source"]]
df.to_csv("AUDUSD_hourly_compare.csv")