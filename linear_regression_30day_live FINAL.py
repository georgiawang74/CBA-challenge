# -*- coding: utf-8 -*-
"""Linear Regression_30day_live.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DGj4VfqeUyfhF1LrgW3e-pEjp2otnsVy
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import Ridge, Lasso
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# Load daily data
CSV_PATH = "final_daily.csv"
df = pd.read_csv(CSV_PATH)

# Ensure Date column is properly formatted
df['Date'] = pd.to_datetime(df['Date'], errors="coerce")
df = df.dropna(subset=['Date']).sort_values('Date').reset_index(drop=True)

target_col = "Close"  # AUDUSD close price
print(f"Dataset: {len(df)} daily observations from {df['Date'].min()} to {df['Date'].max()}")

# === FEATURE ENGINEERING FOR DAILY FREQUENCY ===

# 1. FX lagged features (prevent data leakage)
df["aud_lag1"] = df[target_col].shift(1)
df["aud_lag2"] = df[target_col].shift(2)
df["aud_lag5"] = df[target_col].shift(5)  # week lag

# 2. FX technical indicators (daily frequency)
df["aud_ma20"] = df[target_col].rolling(20, min_periods=10).mean().shift(1)  # monthly MA
df["aud_ma50"] = df[target_col].rolling(50, min_periods=25).mean().shift(1)  # quarterly MA
df["aud_vol10"] = df[target_col].pct_change().rolling(10).std().shift(1)

# 3. Commodity features
commodities = ['Gold_Close', 'Copper_Close', 'Oil_Close', 'NatGas_Close']
for comm in commodities:
    if comm in df.columns:
        # Raw commodity prices (lagged)
        df[f"{comm.lower()}_lag2"] = df[comm].shift(2)

        # Commodity momentum (important for AUD)
        df[f"{comm.lower()}_ret5"] = df[comm].pct_change(5).shift(1)  # 5-day return

        # Commodity vs AUD correlation proxy
        df[f"{comm.lower()}_ratio"] = (df[comm] / df[target_col]).shift(1)

# 4. Relative strength indicators
df["aud_vs_ma20"] = (df[target_col] / df["aud_ma20"] - 1).shift(1)  # deviation from trend
df["ma_momentum"] = (df["aud_ma20"] / df["aud_ma50"] - 1).shift(1)  # trend strength

# 5. Market regime indicators
df["aud_ret1"] = df[target_col].pct_change().shift(1)
df["aud_ret5"] = df[target_col].pct_change(5).shift(1)

# 6. Cyclical features (less important for daily, but can help)
df["day_of_week"] = df['Date'].dt.dayofweek
df["month"] = df['Date'].dt.month

# === FEATURE SELECTION ===
base_features = [
    "aud_lag1", "aud_lag2", "aud_lag5",
    "aud_ma20", "aud_ma50", "aud_vol10",
    "aud_vs_ma20", "ma_momentum",
    "aud_ret1", "aud_ret5"
]

# Add commodity features that exist
commodity_features = []
for comm in commodities:
    for suffix in ['_lag2', '_ret5', '_ratio']:
        feat_name = comm.lower() + suffix
        if feat_name in df.columns:
            commodity_features.append(feat_name)

features = base_features + commodity_features + ["day_of_week", "month"]

# Clean features list (only include columns that exist and have data)
features = [f for f in features if f in df.columns]
print(f"Using {len(features)} features: {features}")

# === TARGET VARIABLE ===
df["y_next"] = df[target_col].shift(-1)  # Next day's close

# === CLEAN DATA ===
dfm = df.dropna(subset=features + ["y_next"]).copy()
X = dfm[features]
y = dfm["y_next"]
dates = dfm["Date"]

print(f"Clean dataset: {len(dfm)} observations with {len(features)} features")

# === WALK-FORWARD FRAMEWORK (adapted for daily) ===
split_idx = int(len(dfm) * 0.7) #index split prevents shuffling of data, extremely important for time series analysis
print(f"Walk-forward testing starts from observation {split_idx} ({len(dfm) - split_idx} predictions)")

# Adjusted parameters for daily frequency
REFIT_EVERY = 30    # retrain monthly (vs 48 hours)
ROLLING_SIZE = 500  # use last ~2 years (vs 2000 hours)
MIN_TRAIN = max(200, int(len(X) * 0.3))  # need less data for linear models

print(f"Daily walk-forward parameters:")
print(f"Refit every: {REFIT_EVERY} days")
print(f"Rolling window: {ROLLING_SIZE} days")
print(f"Minimum training: {MIN_TRAIN} days")

# === MODEL FACTORY FOR LINEAR REGRESSION ===
def create_daily_model(alpha=1.0, model_type='ridge'):
    """Linear model optimized for daily FX prediction"""
    if model_type == 'ridge':
        return Ridge(alpha=alpha, random_state=42)
    elif model_type == 'lasso':
        return Lasso(alpha=alpha, random_state=42, max_iter=2000)
    else:
        raise ValueError("model_type must be 'ridge' or 'lasso'")

# === WALK-FORWARD VALIDATION ===
X_np = X.to_numpy(dtype=np.float32)
y_np = y.to_numpy(dtype=np.float32)

# Standardize features (important for linear regression)
scaler = StandardScaler()

wf_preds, wf_true, wf_dates = [], [], []
model_wf = None
scaler_wf = None

print(f"Starting walk-forward simulation with Ridge regression...")

for i in range(split_idx, len(X_np)):
    if i < MIN_TRAIN:
        continue

    start = max(0, i - ROLLING_SIZE)

    # Refit periodically
    if (i == split_idx) or ((i - split_idx) % REFIT_EVERY == 0) or (model_wf is None):
        # Standardize training data
        scaler_wf = StandardScaler()
        X_train_scaled = scaler_wf.fit_transform(X_np[start:i, :])

        # Train linear model
        model_wf = create_daily_model(alpha=1.0, model_type='ridge')
        model_wf.fit(X_train_scaled, y_np[start:i])

        print(f"  Retrained at step {i}: using data from {start} to {i}")

    # Predict using standardized features
    X_pred_scaled = scaler_wf.transform(X_np[[i-1], :])
    yhat = float(model_wf.predict(X_pred_scaled)[0])

    wf_preds.append(yhat)
    wf_true.append(float(y_np[i]))
    wf_dates.append(dates.iloc[i])

# === EVALUATION ===
if wf_preds:
    wf_preds = np.array(wf_preds)
    wf_true = np.array(wf_true)

    rmse_wf = np.sqrt(mean_squared_error(wf_true, wf_preds))
    mae_wf = mean_absolute_error(wf_true, wf_preds)
    r2_wf = r2_score(wf_true, wf_preds)

    print(f"\n=== Walk-Forward Performance (Daily Ridge Regression) ===")
    print(f"Samples: {len(wf_true)}")
    print(f"RMSE   : {rmse_wf:.6f}")
    print(f"MAE    : {mae_wf:.6f}")
    print(f"R²     : {r2_wf:.4f}")

    # Plotting results
    plt.figure(figsize=(12,5))
    plt.plot(wf_dates, wf_true, label="Actual next day", linewidth=1.5)
    plt.plot(wf_dates, wf_preds, label="Predicted next day", linestyle="--", linewidth=1.5)
    plt.title("AUDUSD Daily Spot — Walk-Forward Predictions")
    plt.xlabel("Date"); plt.ylabel("Spot"); plt.legend(); plt.grid(True)
    plt.gcf().autofmt_xdate(); plt.show()

    plt.figure(figsize=(6,6))
    plt.scatter(wf_true, wf_preds, alpha=0.6)
    lo, hi = float(min(wf_true.min(), wf_preds.min())), float(max(wf_true.max(), wf_preds.max()))
    plt.plot([lo, hi], [lo, hi], linestyle="--", color='red', linewidth=2)
    plt.xlabel("Actual Spot"); plt.ylabel("Predicted Spot")
    plt.title("Actual vs Predicted Daily Spot (walk-forward)")
    plt.grid(True); plt.show()

    # Calculate pip equivalent (for comparison with hourly model)
    resid_pips = (wf_true - wf_preds) * 1e4
    sigma_pips = float(np.std(resid_pips))
    print(f"Prediction error: ±{sigma_pips:.1f} pips (1 std)")

    # Feature importance (linear regression coefficients)
    if model_wf is not None:
        coefficients = pd.Series(model_wf.coef_, index=features).abs().sort_values(ascending=False)
        print(f"\n=== Feature Importance (|Coefficients|) ===")
        print(coefficients.head(10))


    # === SIGNAL GENERATION FOR NEXT DAY ===
    import math, json, os
    os.makedirs("outputs", exist_ok=True)

    # 1) sigma_pips already calculated above from walk-forward residuals

    # 2) Fit a fresh model on the most recent rolling window and predict the *next* day
    start = max(0, len(X) - ROLLING_SIZE)

    # Train fresh model using same architecture
    scaler_now = StandardScaler()
    X_train_scaled = scaler_now.fit_transform(X.iloc[start:])
    model_now = create_daily_model(alpha=1.0, model_type='ridge')
    model_now.fit(X_train_scaled, y.iloc[start:])

    last_spot = float(df[target_col].iloc[-1])  # current price
    X_pred_scaled = scaler_now.transform(X.iloc[[-1]])
    yhat_next = float(model_now.predict(X_pred_scaled)[0])  # next-day forecast
    pred_pips = (yhat_next - last_spot) * 1e4 #predicted movement mesasured in pips

    # 3) Noise gate (costs + uncertainty) + momentum filter (adapted for daily)
    SPREAD_PIPS = 1.0 #transaction cost
    K_COST, K_SIGMA = 2.0, 1.0  # Higher thresholds for daily (K_COST, is a multiplier which inflates raw spread to cover both sides of trade, slippage, latency)
    threshold = K_COST * SPREAD_PIPS + K_SIGMA * sigma_pips

    # Daily momentum filter (look at last week instead of last 3 hours)
    r = df[target_col].pct_change()
    recent_mo = float(r.tail(5).sum())  # last 5 days momentum
    mo_ok = (np.sign(pred_pips) == np.sign(recent_mo)) or (abs(pred_pips)/(sigma_pips+1e-9) >= 1.5)

    signal = "flat"
    if (abs(pred_pips) > threshold) and mo_ok:
        signal = "long" if pred_pips > 0 else "short"

    # 4) Confidence (probability the move beats costs under Normal residuals)
    z_cost = (abs(pred_pips) - SPREAD_PIPS) / (sigma_pips + 1e-9)
    p_edge = 0.5 * (1.0 + math.erf(z_cost / math.sqrt(2)))
    confidence = float(np.clip(2*(p_edge - 0.5), 0, 1))

    # 5) TP/SL from predicted move + residual sigma (adapted for daily)
    K_TP, K_SL = 1.5, 1.0  # Wider stops for daily
    tp = sl = None
    if signal != "flat":
        tp = last_spot + (np.sign(pred_pips) * (abs(pred_pips) + K_TP*sigma_pips)) / 1e4
        sl = last_spot + (-np.sign(pred_pips) * K_SL*sigma_pips) / 1e4

    print(f"\n=== Current Daily Trading Signal ===")
    print(f"Signal: {signal} | Confidence: {confidence:.2f} | Pred move: {pred_pips:.1f} pips | "
          f"sigma≈{sigma_pips:.1f} | threshold≈{threshold:.1f}")

    # 6) Save daily signal
    out = {
      "timestamp": str(df['Date'].iloc[-1]),
      "spot": round(last_spot, 6),
      "pred_next_day": round(yhat_next, 6),
      "pred_move_pips": round(pred_pips, 1),
      "signal": signal,
      "confidence": round(confidence, 2),
      "tp": (round(tp, 6) if tp is not None else None),
      "sl": (round(sl, 6) if sl is not None else None),
      "assumptions": {
        "spread_pips": SPREAD_PIPS,
        "sigma_pips": round(sigma_pips, 1),
        "momentum_ok": bool(mo_ok),
        "timeframe": "daily"
      }
    }
    json.dump(out, open("outputs/latest_daily_signal.json","w"), indent=2)
    print("Saved daily signal -> outputs/latest_daily_signal.json")



else:
    print("\nWalk-forward skipped (not enough data after warmup).")

# === 30-DAY HORIZON PREDICTION ===
def train_daily_horizon_model(X, y, horizon_days, features, model_type='ridge'):
    """Train linear model for specific daily horizon using walk-forward validation"""

    y_h = y.shift(-horizon_days)
    valid_mask = (~X.isna().any(axis=1)) & (~y_h.isna())
    X_h = X.loc[valid_mask]
    y_h = y_h.loc[valid_mask]

    if len(X_h) < 200:  # Need less data for linear models
        return None, None, None

    split_idx = int(len(X_h) * 0.7)
    REFIT_EVERY = 30
    ROLLING_SIZE = 500
    MIN_TRAIN = max(200, int(len(X_h) * 0.3))

    X_np = X_h.to_numpy(dtype=np.float32)
    y_np = y_h.to_numpy(dtype=np.float32)

    wf_preds, wf_true = [], []
    model_h = None
    scaler_h = None

    for i in range(split_idx, len(X_np)):
        if i < MIN_TRAIN:
            continue

        start = max(0, i - ROLLING_SIZE)

        if (i == split_idx) or ((i - split_idx) % REFIT_EVERY == 0) or (model_h is None):
            scaler_h = StandardScaler()
            X_train_scaled = scaler_h.fit_transform(X_np[start:i, :])
            model_h = create_daily_model(alpha=1.0, model_type=model_type)
            model_h.fit(X_train_scaled, y_np[start:i])

        X_pred_scaled = scaler_h.transform(X_np[[i-1], :])
        yhat = float(model_h.predict(X_pred_scaled)[0])
        wf_preds.append(yhat)
        wf_true.append(float(y_np[i]))

    if len(wf_preds) > 0:
        wf_preds = np.array(wf_preds)
        wf_true = np.array(wf_true)
        r2_h = r2_score(wf_true, wf_preds)
        mae_h = mean_absolute_error(wf_true, wf_preds)

        # Train final model on most recent data
        final_start = max(0, len(X_h) - ROLLING_SIZE)
        final_scaler = StandardScaler()
        X_final_scaled = final_scaler.fit_transform(X_h.iloc[final_start:])

        final_model = create_daily_model(alpha=1.0, model_type=model_type)
        final_model.fit(X_final_scaled, y_h.iloc[final_start:])

        return (final_model, final_scaler), r2_h, mae_h

    return None, None, None

# Train models for key horizons
key_horizons = list(range(1,31)) # Daily horizons
horizon_models = {}
horizon_performance = {}

print("\nTraining daily horizon models using walk-forward methodology:")

for h in key_horizons:
    print(f"\nTraining {h}-day model...")
    model_result, r2_h, mae_h = train_daily_horizon_model(X, y, h, features)

    if model_result is not None:
        horizon_models[h] = model_result
        horizon_performance[h] = {'r2': r2_h, 'mae': mae_h}
        print(f"  {h}d model: R² = {r2_h:.3f}, MAE = {mae_h:.6f}")
    else:
        print(f"  {h}d model: Insufficient data")

# Generate path using validated models
print(f"\n=== Generating 30d Path ===")

if len(horizon_models) > 0:
    current_features = X.iloc[[-1]]
    current_spot = float(df[target_col].iloc[-1])

    path_predictions = []

    for d in range(1, 31):  # 30 days
        if d in horizon_models:
            # Use validated model for this horizon
            model_h, scaler_h = horizon_models[d]
            current_scaled = scaler_h.transform(current_features)
            pred_spot = float(model_h.predict(current_scaled)[0])
        else:
            # Interpolate between validated models - using linear interpolation (I think for less granular data it is more acceptable but still not the best), FX is not linear
            lower_d = max([k for k in horizon_models.keys() if k <= d], default=1)
            upper_d = min([k for k in horizon_models.keys() if k >= d], default=30)

            if lower_d == upper_d:
                model_h, scaler_h = horizon_models[lower_d]
                current_scaled = scaler_h.transform(current_features)
                pred_spot = float(model_h.predict(current_scaled)[0])
            else:
                # Linear interpolation between validated models
                model_l, scaler_l = horizon_models[lower_d]
                model_u, scaler_u = horizon_models[upper_d]

                current_scaled_l = scaler_l.transform(current_features)
                current_scaled_u = scaler_u.transform(current_features)

                lower_pred = float(model_l.predict(current_scaled_l)[0])
                upper_pred = float(model_u.predict(current_scaled_u)[0])

                weight = (d - lower_d) / (upper_d - lower_d)
                pred_spot = lower_pred + weight * (upper_pred - lower_pred)

        move_pips = (pred_spot - current_spot) * 1e4

        path_predictions.append({
            'horizon_days': d,
            'predicted_spot': pred_spot,
            'move_pips': move_pips,
            'model_type': 'validated' if d in horizon_models else 'interpolated'
        })

    path_forecast = pd.DataFrame(path_predictions)

    # Signal generation - ADJUSTED FOR DAILY TIMEFRAME
    SPREAD_PIPS = 1.0
    DAILY_THRESH = 0.25*sigma_pips   # sigma_pips seems to have the largest effect on how threshold is determined but if we use unscaled value, signals all flat
    path_forecast["signal"] = np.where(
        abs(path_forecast["move_pips"]) > DAILY_THRESH,
        np.where(path_forecast["move_pips"] > 0, "long", "short"),
        "flat"
    )
    path_forecast["confidence"] = np.clip(
        abs(path_forecast["move_pips"]) / (2*sigma_pips + 1e-9), 0, 1
    )

    # Save using same approach as original
    path_forecast.to_csv("outputs/next30_daily_forecast.csv", index=False)
    path_forecast.to_csv("outputs/next30_daily_forecast_with_signals.csv", index=False)

    print("Saved -> outputs/next30_daily_forecast_with_signals.csv")
    print(path_forecast.head(10))

    # Plot using enhanced approach
    y_forecast = path_forecast["predicted_spot"].values
    x_forecast = path_forecast["horizon_days"].values

    plt.figure(figsize=(12,6))

    # Color-code by model type
    validated_mask = path_forecast['model_type'] == 'validated'

    plt.plot(x_forecast[validated_mask], y_forecast[validated_mask],
             'o-', linewidth=2, markersize=6, label="Walk-Forward Validated", color='blue')
    plt.plot(x_forecast[~validated_mask], y_forecast[~validated_mask],
             'o-', linewidth=1, markersize=4, label="Interpolated", color='green', alpha=0.7)


    # Show validation points
    for d in horizon_models.keys():
        plt.axvline(x=d, color='green', alpha=0.3, linestyle=':')

    price_range = y_forecast.max() - y_forecast.min()
    plt.title(f"30-Day Path Using Walk-Forward Methodology\nRange: {price_range*1e4:.1f} pips")
    plt.xlabel("Days Ahead")
    plt.ylabel("Predicted Spot")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # Performance summary
    print(f"\n=== Model Performance Summary ===")
    for d, perf in horizon_performance.items():
        print(f"  {d}d model: R² = {perf['r2']:.3f} (walk-forward validated)")

    print(f"Signal logic: threshold system (±{DAILY_THRESH:.1f} pips)")
    print(f"Same features: {features}")

else:
    print("No horizon models could be trained - insufficient data")